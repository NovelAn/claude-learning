#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹çš„å†…å®¹åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºå¯¹å·²æŠ“å–çš„æ–‡ç« è¿›è¡Œæ™ºèƒ½æ–‡æœ¬åˆ†æ
ä¸è°ƒç”¨æè‡´äº†APIï¼Œåªä½¿ç”¨OpenAIè¿›è¡Œå†…å®¹åˆ†æ
"""

import sys
import json
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from content_analyzer import ContentAnalyzer
from config import DEEPSEEK_CONFIG, OPENAI_CONFIG, DATA_CONFIG
import glob


def analyze_article_file(article_file: str) -> dict:
    """
    åˆ†æå•ä¸ªæ–‡ç« JSONæ–‡ä»¶

    Args:
        article_file: æ–‡ç« JSONæ–‡ä»¶è·¯å¾„

    Returns:
        dict: åŒ…å«åŸå§‹æ•°æ®å’Œå†…å®¹åˆ†æçš„ç»“æœ
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“„ åˆ†ææ–‡ä»¶: {Path(article_file).name}")
    print('='*80)

    # è¯»å–æ–‡ç« æ•°æ®
    with open(article_file, 'r', encoding='utf-8') as f:
        article_data = json.load(f)

    print(f"ğŸ“ æ ‡é¢˜: {article_data.get('title', 'N/A')[:60]}...")
    print(f"ğŸ¢ è´¦å·: {article_data.get('account_name', 'N/A')}")

    # åˆ›å»ºå†…å®¹åˆ†æå™¨ï¼ˆä¼˜å…ˆä½¿ç”¨DeepSeekï¼‰
    llm_config = None
    if DEEPSEEK_CONFIG.get('api_key'):
        llm_config = DEEPSEEK_CONFIG
        llm_provider = 'DeepSeek'
    elif OPENAI_CONFIG.get('api_key'):
        llm_config = OPENAI_CONFIG
        llm_provider = 'OpenAI'

    analyzer = ContentAnalyzer(
        api_key=llm_config.get('api_key') if llm_config else None,
        base_url=llm_config.get('base_url') if llm_config else None,
        model=llm_config.get('model') if llm_config else None
    )

    # æ‰§è¡Œå†…å®¹åˆ†æ
    analysis_result = analyzer.analyze_article(article_data)

    # åˆå¹¶æ•°æ®
    result = {
        'article_data': article_data,
        'content_analysis': analysis_result
    }

    return result


def analyze_batch(article_dir: str = None, pattern: str = '*.json') -> list:
    """
    æ‰¹é‡åˆ†ææ–‡ç« 

    Args:
        article_dir: æ–‡ç« ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨DATA_CONFIGé…ç½®
        pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼

    Returns:
        list: åˆ†æç»“æœåˆ—è¡¨
    """
    if article_dir is None:
        article_dir = DATA_CONFIG['articles_dir']

    article_files = glob.glob(str(Path(article_dir) / pattern))

    if not article_files:
        print(f"âŒ æœªæ‰¾åˆ°æ–‡ç« æ–‡ä»¶: {article_dir}/{pattern}")
        return []

    print(f"\nğŸ” æ‰¾åˆ° {len(article_files)} ç¯‡æ–‡ç« å¾…åˆ†æ")
    print("="*80)

    results = []
    for i, article_file in enumerate(article_files, 1):
        print(f"\n[{i}/{len(article_files)}] å¤„ç†ä¸­...")
        try:
            result = analyze_article_file(article_file)
            results.append(result)
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            continue

    return results


def save_analysis_results(results: list, output_file: str = None):
    """
    ä¿å­˜åˆ†æç»“æœ

    Args:
        results: åˆ†æç»“æœåˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if output_file is None:
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path(DATA_CONFIG.get('reports_dir', 'reports'))
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f'content-analysis-{timestamp}.json'

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def print_summary(results: list):
    """
    æ‰“å°åˆ†ææ‘˜è¦

    Args:
        results: åˆ†æç»“æœåˆ—è¡¨
    """
    print("\n" + "="*80)
    print("ğŸ“Š å†…å®¹åˆ†ææ‘˜è¦")
    print("="*80)

    total_articles = len(results)
    total_insights = sum(len(r.get('content_analysis', {}).get('key_insights', []))
                         for r in results)
    total_data_points = sum(len(r.get('content_analysis', {}).get('data_points', []))
                            for r in results)
    total_entities = sum(len(r.get('content_analysis', {}).get('entities', []))
                        for r in results)

    print(f"âœ… æˆåŠŸåˆ†æ: {total_articles} ç¯‡æ–‡ç« ")
    print(f"ğŸ’¡ æå–è§‚ç‚¹: {total_insights} æ¡")
    print(f"ğŸ“Š æ ‡æ³¨æ•°æ®: {total_data_points} ä¸ª")
    print(f"ğŸ¢ è¯†åˆ«å®ä½“: {total_entities} ä¸ª")

    print("\nğŸ“‹ æ–‡ç« åˆ—è¡¨:")
    for i, result in enumerate(results, 1):
        article = result.get('article_data', {})
        analysis = result.get('content_analysis', {})
        print(f"\n{i}. {article.get('title', 'N/A')[:50]}...")
        print(f"   æ‘˜è¦é•¿åº¦: {len(analysis.get('summary', ''))} å­—ç¬¦")
        print(f"   æ ¸å¿ƒè§‚ç‚¹: {len(analysis.get('key_insights', []))} æ¡")
        print(f"   å…³é”®æ•°æ®: {len(analysis.get('data_points', []))} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ğŸ¯ å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ™ºèƒ½å†…å®¹åˆ†æå·¥å…·                      â•‘
â•‘         (ç‹¬ç«‹ç‰ˆ - ä»…å†…å®¹åˆ†æï¼Œä¸è°ƒç”¨æ•°æ®API)                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # æ£€æŸ¥APIå¯†é’¥
    has_llm_api = bool(DEEPSEEK_CONFIG.get('api_key') or OPENAI_CONFIG.get('api_key'))
    if not has_llm_api:
        print("âš ï¸  è­¦å‘Š: æœªé…ç½®LLM APIå¯†é’¥")
        print("   å†…å®¹åˆ†æåŠŸèƒ½éœ€è¦DeepSeek APIï¼ˆæ¨èï¼‰æˆ–OpenAI API")
        print("   è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®: DEEPSEEK_API_KEY=your-key-here\n")
        print("   ä½ ä»ç„¶å¯ä»¥è¿è¡Œï¼Œä½†ä¼šä½¿ç”¨é™çº§æ–¹æ¡ˆï¼ˆåŸºç¡€è§„åˆ™æå–ï¼‰\n")

    print("è¯·é€‰æ‹©æ“ä½œ:")
    print("1. åˆ†æå•ç¯‡æ–‡ç« ")
    print("2. æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ç« ")
    print("3. åˆ†ææœ€æ–°çš„ä¸€ç¯‡æ–‡ç« ")
    print("4. é€€å‡º")

    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4): ").strip()

    if choice == '1':
        # åˆ†æå•ç¯‡æ–‡ç« 
        file_path = input("è¯·è¾“å…¥æ–‡ç« JSONæ–‡ä»¶è·¯å¾„: ").strip()
        if not Path(file_path).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        result = analyze_article_file(file_path)
        save_analysis_results([result])

        # æ‰“å°è¯¦ç»†æŠ¥å‘Š
        analyzer = ContentAnalyzer()
        print("\n" + analyzer.format_analysis_report(result['content_analysis']))

    elif choice == '2':
        # æ‰¹é‡åˆ†æ
        print("\nå¼€å§‹æ‰¹é‡åˆ†ææ‰€æœ‰æ–‡ç« ...")
        results = analyze_batch()
        if results:
            save_analysis_results(results)
            print_summary(results)

    elif choice == '3':
        # åˆ†ææœ€æ–°æ–‡ç« 
        article_dir = DATA_CONFIG['articles_dir']
        article_files = glob.glob(str(Path(article_dir) / '*.json'))

        if not article_files:
            print(f"âŒ æœªæ‰¾åˆ°æ–‡ç« æ–‡ä»¶: {article_dir}")
            return

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        latest_file = max(article_files, key=lambda f: Path(f).stat().st_mtime)
        print(f"\nğŸ” æ‰¾åˆ°æœ€æ–°æ–‡ç« : {Path(latest_file).name}")

        result = analyze_article_file(latest_file)
        save_analysis_results([result])

        # æ‰“å°è¯¦ç»†æŠ¥å‘Š
        analyzer = ContentAnalyzer()
        print("\n" + analyzer.format_analysis_report(result['content_analysis']))

    elif choice == '4':
        print("ğŸ‘‹ å†è§!")
        return

    else:
        print("âŒ æ— æ•ˆçš„é€‰é¡¹")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
