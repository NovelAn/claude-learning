#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨
ç»“åˆæè‡´äº†APIè·å–å®Œæ•´çš„æ–‡ç« å†…å®¹å’Œäº’åŠ¨æ•°æ®
"""

import json
import os
import time
import random
from datetime import datetime
from typing import Dict, Optional, List
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import requests
from bs4 import BeautifulSoup
import re
from config import JIZHILA_API, DATA_CONFIG, ERROR_MESSAGES

class WeChatArticleFetcher:
    """å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç»¼åˆæŠ“å–å™¨"""

    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.api_key = api_key or JIZHILA_API['key']
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        self.articles_dir = DATA_CONFIG['articles_dir']
        self.reports_dir = DATA_CONFIG['reports_dir']

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.articles_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)

    def fetch_article(self, url: str) -> Optional[Dict]:
        """
        æŠ“å–å•ç¯‡å¾®ä¿¡æ–‡ç« çš„å®Œæ•´æ•°æ®

        Args:
            url: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URL

        Returns:
            åŒ…å«æ–‡ç« å†…å®¹å’Œäº’åŠ¨æ•°æ®çš„å­—å…¸
        """
        print(f"ğŸ“‡ æ­£åœ¨æŠ“å–æ–‡ç« : {url}")

        try:
            # 1. è·å–æ–‡ç« å†…å®¹
            article_data = self._get_article_content(url)
            if not article_data:
                print("âŒ æ–‡ç« å†…å®¹æŠ“å–å¤±è´¥")
                return None

            # 2. è·å–äº’åŠ¨æ•°æ®ï¼ˆä¾èµ–APIï¼‰
            if self.api_key:
                print("ğŸ“Š æ­£åœ¨é€šè¿‡æè‡´äº†APIè·å–äº’åŠ¨æ•°æ®...")
                interaction_data = self._get_interaction_data(url)
                if interaction_data:
                    article_data['interaction_data'] = interaction_data

                    # 3. è®¡ç®—çƒ­åº¦æŒ‡æ•°
                    article_data['hot_index'] = self._calculate_hot_index(article_data, interaction_data)
                    article_data['interaction_status'] = 'success'
                    print(f"âœ… äº’åŠ¨æ•°æ®è·å–æˆåŠŸ")
                    print(f"   ğŸ“– é˜…è¯»æ•°: {interaction_data['read_count']:,}")
                    print(f"   ğŸ‘ ç‚¹èµæ•°: {interaction_data['like_count']:,}")
                    print(f"   ğŸ“ˆ çƒ­åº¦æŒ‡æ•°: {article_data['hot_index']}/100")
                else:
                    print("âš ï¸  äº’åŠ¨æ•°æ®è·å–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                    article_data['interaction_status'] = 'failed'
                    article_data['hot_index'] = self._calculate_hot_index_without_interaction(article_data)
            else:
                print("âš ï¸  æœªé…ç½®APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿäº’åŠ¨æ•°æ®")
                article_data['interaction_status'] = 'mock'
                mock_data = self._generate_mock_interaction_data(article_data)
                article_data['interaction_data'] = mock_data
                article_data['hot_index'] = self._calculate_hot_index(article_data, mock_data)

            # 4. ä¿å­˜æ•°æ®
            article_data['fetched_at'] = datetime.now().isoformat()
            self._save_article_data(article_data)

            return article_data

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return None

    def _get_article_content(self, url: str) -> Optional[Dict]:
        """è·å–æ–‡ç« å†…å®¹"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…IPè¢«å°
            time.sleep(random.uniform(0.5, 1.5))

            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–å„é¡¹å†…å®¹
            title = self._extract_title(soup)
            author = self._extract_author(soup)
            content = self._extract_content(soup)
            publish_time = self._extract_publish_time(soup)
            account_name = self._extract_account_name(soup)

            article_data = {
                'url': url,
                'title': title,
                'author': author,
                'content': content,
                'content_text': BeautifulSoup(content, 'html.parser').get_text(strip=True),
                'account_name': account_name,
                'publish_time': publish_time,
                'snapshot_time': datetime.now().isoformat()
            }

            # åˆæ­¥åˆ†æ
            article_data.update(self._analyze_content(article_data['content_text']))

            print(f"âœ… æ–‡ç« å†…å®¹æŠ“å–å®Œæˆ")
            print(f"   ğŸ“ æ ‡é¢˜: {title[:50]}...")
            print(f"   ğŸ‘¤ ä½œè€…: {author}")
            print(f"   ğŸ¢ è´¦å·: {account_name}")
            print(f"   ğŸ“ å†…å®¹é•¿åº¦: {len(article_data['content_text'])} å­—ç¬¦")

            return article_data

        except Exception as e:
            print(f"âŒ å†…å®¹æŠ“å–å¤±è´¥: {e}")
            return None

    def _get_interaction_data(self, url: str) -> Optional[Dict]:
        """é€šè¿‡æè‡´äº†APIè·å–äº’åŠ¨æ•°æ®"""
        if not self.api_key:
            print("âŒ æœªé…ç½®APIå¯†é’¥")
            return None

        try:
            # APIè°ƒç”¨
            params = {
                'key': self.api_key,
                'url': url
            }

            response = requests.get(
                JIZHILA_API['url'],
                params=params,
                timeout=30
            )
            response.raise_for_status()

            result = response.json()

            # æè‡´äº†APIæˆåŠŸåˆ¤æ–­ï¼šcode == 1 æˆ– code == 0 éƒ½è¡¨ç¤ºæˆåŠŸ
            if result.get('code') in [0, 1]:
                data = result.get('data', {})

                # æ ‡å‡†æ•°æ®ç»“æ„
                interaction_data = {
                    'read_count': data.get('read', 0),
                    'like_count': data.get('zan', 0),
                    'share_count': data.get('share_num', 0),
                    'collect_count': data.get('collect_num', 0),
                    'comment_count': data.get('comment_count', 0),
                    'data_source': 'jizhila_api',
                    'api_response': data,
                    'confidence': 'high',
                    'notes': 'æ¥è‡ªæè‡´äº†æ•°æ®API'
                }

                # è®¡ç®—äº’åŠ¨ç‡
                if interaction_data['read_count'] > 0:
                    interaction_data['like_rate'] = interaction_data['like_count'] / interaction_data['read_count']
                    interaction_data['share_rate'] = interaction_data['share_count'] / interaction_data['read_count']
                    interaction_data['collect_rate'] = interaction_data['collect_count'] / interaction_data['read_count']
                    interaction_data['comment_rate'] = interaction_data['comment_count'] / interaction_data['read_count']

                return interaction_data

            else:
                print(f"APIè¿”å›é”™è¯¯: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                return None

        except requests.exceptions.RequestException as e:
            print(f"APIè¯·æ±‚å¤±è´¥: {e}")
            return None

    def _generate_mock_interaction_data(self, article_data: Dict) -> Dict:
        """ç”Ÿæˆæ¨¡æ‹Ÿäº’åŠ¨æ•°æ®"""
        # åŸºäºæ–‡ç« ç‰¹å¾ç”Ÿæˆåˆç†çš„æ¨¡æ‹Ÿæ•°æ®
        content_length = len(article_data.get('content_text', ''))
        title = article_data.get('title', '')

        # å®‰å…¨å¤„ç†å‘å¸ƒæ—¶é—´
        publish_time_str = article_data.get('publish_time', '')
        try:
            if publish_time_str:
                published_ago = (datetime.now() - datetime.fromisoformat(publish_time_str)).days
            else:
                published_ago = 0  # é»˜è®¤ä¸ºä»Šå¤©å‘å¸ƒ
        except:
            published_ago = 0  # è§£æå¤±è´¥ï¼Œé»˜è®¤ä¸ºä»Šå¤©å‘å¸ƒ

        # åŸºç¡€é˜…è¯»æ•°ï¼ˆè€ƒè™‘å†…å®¹é•¿åº¦å’Œæ—¶æ•ˆæ€§ï¼‰
        base_read = max(5000, min(content_length * 10, 80000))
        if published_ago > 7:
            base_read *= 0.8  # è¶…è¿‡ä¸€å‘¨çš„æ–‡ç« ï¼Œé˜…è¯»é‡åšä¹˜æ³•å‡å°‘

        # é¢†åŸŸè°ƒæ•´
        topic_boost = 1.0
        if any(word in title for word in ['æ—¶å°š', 'å¥¢å“', 'å¥¢ä¾ˆå“', 'Ferragamo', 'LVMH']):
            topic_boost = 1.8
        elif any(word in title for word in ['å•†ä¸š', 'è´¢æŠ¥', 'æ”¶è´­']):
            topic_boost = 1.4
        elif any(word in title for word in ['ç¾å¦†', 'ç¾å®¹']):
            topic_boost = 1.6

        read_count = int(base_read * topic_boost)

        # äº’åŠ¨æ•°æ®
        like_count = int(read_count * random.uniform(0.015, 0.035))  # 1.5%-3.5%ç‚¹èµç‡
        share_count = int(read_count * random.uniform(0.003, 0.008))  # 0.3%-0.8%åˆ†äº«ç‡
        collect_count = int(read_count * random.uniform(0.003, 0.008))  # 0.3%-0.8%æ”¶è—ç‡
        comment_count = int(read_count * random.uniform(0.005, 0.012))  # 0.5%-1.2%è¯„è®ºç‡

        return {
            'read_count': read_count,
            'like_count': like_count,
            'share_count': share_count,
            'comment_count': comment_count,
            'collect_count': collect_count,
            'data_source': 'simulation',
            'confidence': 'low',
            'notes': 'åŸºäºæ–‡ç« ç‰¹å¾ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®'
        }

    def _calculate_hot_index(self, article_data: Dict, interaction_data: Dict) -> int:
        """è®¡ç®—æ–‡ç« çƒ­åº¦æŒ‡æ•° (0-100)"""
        score = 0

        # 1. é˜…è¯»é‡æƒé‡ (50åˆ†)
        read_count = interaction_data.get('read_count', 0)
        if read_count >= 100000: score += 50
        elif read_count >= 50000: score += 40
        elif read_count >= 20000: score += 30
        elif read_count >= 10000: score += 20
        elif read_count >= 5000: score += 10
        else: score += 0

        # 2. äº’åŠ¨ç‡æƒé‡ (30åˆ†)
        read_count = max(read_count, 1)  # é¿å…é™¤é›¶
        like_rate = interaction_data.get('like_count', 0) / read_count
        share_rate = interaction_data.get('share_count', 0) / read_count

        if like_rate >= 0.05: score += 20  # 5%+ç‚¹èµç‡
        elif like_rate >= 0.03: score += 15
        elif like_rate >= 0.02: score += 10
        else: score += 5

        if share_rate >= 0.01: score += 10  # 1%+åˆ†äº«ç‡
        elif share_rate >= 0.005: score += 7
        elif share_rate >= 0.003: score += 5
        else: score += 0

        # 3. å†…å®¹ä»·å€¼æƒé‡ (15åˆ†)
        title_score = 0
        title = article_data.get('title', '')
        for keyword in ['æ—¶å°š', 'å¥¢ä¾ˆå“', 'è´¢æŠ¥', 'æ”¶è´­', 'CEO', 'æ•°æ®']:
            if keyword in title:
                title_score += 2
        score += min(title_score, 15)

        # 4. æ—¶æ•ˆæ€§æƒé‡ (5åˆ†)
        publish_time = datetime.fromisoformat(article_data.get('publish_time', datetime.now().isoformat()))
        days_old = (datetime.now() - publish_time).days
        if days_old <= 1: score += 5
        elif days_old <= 3: score += 3
        elif days_old <= 7: score += 1

        return min(score, 100)

    def _calculate_hot_index_without_interaction(self, article_data: Dict) -> int:
        """æ— äº’åŠ¨æ•°æ®æ—¶çš„çƒ­åº¦ä¼°ç®—"""
        score = 30  # åŸºç¡€åˆ†

        # åŸºäºå†…å®¹åˆ†æ
        topics = article_data.get('content_analysis', {}).get('key_topics', [])
        if len(topics) > 5: score += 10
        if article_data.get('tone_analysis') == 'positive': score += 10

        # å†…å®¹é•¿åº¦
        content_length = len(article_data.get('content_text', ''))
        if content_length > 2000: score += 15
        elif content_length > 1000: score += 10
        else: score += 5

        return min(score, 100)

    def _save_article_data(self, article_data: Dict):
        """ä¿å­˜æ–‡ç« æ•°æ®"""
        # ç”Ÿæˆæ–‡ä»¶å
        from urllib.parse import urlparse
        import hashlib

        url_hash = hashlib.md5(article_data['url'].encode()).hexdigest()[:10]
        filename = f"article-{url_hash}-{datetime.now().strftime('%Y%m%d')}.json"
        filepath = os.path.join(self.articles_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(article_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {filepath}")

    def fetch_multiple(self, urls: List[str]) -> List[Dict]:
        """æ‰¹é‡æŠ“å–æ–‡ç« """
        results = []
        print(f"\nğŸ“š å¼€å§‹æ‰¹é‡æŠ“å– {len(urls)} ç¯‡æ–‡ç« ...")

        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] æ­£åœ¨å¤„ç†...")
            result = self.fetch_article(url)
            if result:
                results.append(result)

            # é—´éš”å¤„ç†ï¼Œé¿å…å°å·
            if i < len(urls):
                time.sleep(random.uniform(2, 4))

        print(f"\nâœ… æ‰¹é‡æŠ“å–å®Œæˆ: {len(results)}/{len(urls)} ç¯‡æ–‡ç« ")
        return results

    # è¾…åŠ©æå–æ–¹æ³•
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        selectors = ['h1', '.rich_media_title', 'title']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text().strip()[:200]
        return 'æ— æ ‡é¢˜'

    def _extract_author(self, soup: BeautifulSoup) -> str:
        """æå–ä½œè€…"""
        selectors = ['#js_name', '.rich_media_meta_text', '.profile_nickname']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text().strip()[:50]
        return 'æœªçŸ¥ä½œè€…'

    def _extract_content(self, soup: BeautifulSoup) -> str:
        """æå–å†…å®¹"""
        selectors = ['#js_content', '.rich_media_content']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return str(elem)
        return ''

    def _extract_publish_time(self, soup: BeautifulSoup) -> str:
        """æå–å‘å¸ƒæ—¶é—´"""
        selectors = ['#publish_time', '.rich_media_meta_date', '#post-date']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text().strip()
        return datetime.now().isoformat()

    def _extract_account_name(self, soup: BeautifulSoup) -> str:
        """æå–å…¬ä¼—å·åç§°"""
        # å¾®ä¿¡æ–‡ç« é¡µé¢ç»“æ„
        account_elem = soup.select_one('.rich_media_meta_list a') or soup.select_one('#js_name')
        if account_elem:
            return account_elem.get_text().strip()[:100]
        return 'æœªçŸ¥å…¬ä¼—å·'

    def _analyze_content(self, content_text: str) -> Dict:
        """åˆ†æå†…å®¹"""
        analysis = {
            'content_analysis': {
                'char_count': len(content_text),
                'word_count': len(content_text.split()),
                'paragraph_count': content_text.count('\n\n') + 1
            },
            'key_topics': [],
            'tone_analysis': 'neutral'
        }

        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'[\u4e00-\u9fa5]{2,6}', content_text)
        word_freq = {}
        for word in words:
            if len(word) >= 2:
                word_freq[word] = word_freq.get(word, 0) + 1

        # æŒ‰é¢‘ç‡æ’åºï¼Œå–å‰15ä¸ª
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
        analysis['key_topics'] = [{'term': w[0], 'weight': w[1]/max(word_freq.values()) if word_freq else 0}
                                  for w in top_words]

        # æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        positive_words = ['å¥½æ¶ˆæ¯', 'å¢é•¿', 'æˆåŠŸ', 'ç›ˆåˆ©', 'æå‡', 'åˆ›æ–°', 'çªç ´']
        negative_words = ['ä¸‹æ»‘', 'äºæŸ', 'å›°å¢ƒ', 'å±æœº', 'è¡°é€€', 'å¤±è´¥', 'é—®é¢˜']

        pos_count = sum(content_text.count(word) for word in positive_words)
        neg_count = sum(content_text.count(word) for word in negative_words)

        if pos_count > neg_count * 1.5:
            analysis['tone_analysis'] = 'positive'
        elif neg_count > pos_count * 1.5:
            analysis['tone_analysis'] = 'negative'
        else:
            analysis['tone_analysis'] = 'neutral'

        return analysis


if __name__ == '__main__':
    # æµ‹è¯•ç¤ºä¾‹
    fetcher = WeChatArticleFetcher()

    # æµ‹è¯•æ–‡ç« 
    test_urls = [
        "https://mp.weixin.qq.com/s/nNhtCWVzgkv6vbPyR-JOVQ",    # Ferragamoæ–‡ç« 
        "https://mp.weixin.qq.com/s/Ggf3HzkOp8AjCHp-kgo1dw"     # Lululemonæ–‡ç« 
    ]

    print("ğŸš€ å¼€å§‹æµ‹è¯•/wechat-dailyæ–‡ç« æŠ“å–åŠŸèƒ½...")
    print("å¦‚æœæœªé…ç½®APIå¯†é’¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¼”ç¤º...\n")

    results = fetcher.fetch_multiple(test_urls)

    if results:
        print("\n" + "="*60)
        print("ğŸ“Š æŠ“å–æ±‡æ€»æŠ¥å‘Š")
        print("="*60)

        total_reads = sum(r.get('interaction_data', {}).get('read_count', 0) for r in results)
        total_likes = sum(r.get('interaction_data', {}).get('like_count', 0) for r in results)

        print(f"æˆåŠŸæŠ“å–: {len(results)} ç¯‡æ–‡ç« ")
        print(f"æ€»é˜…è¯»é‡: {total_reads:,}")
        print(f"æ€»ç‚¹èµæ•°: {total_likes:,}")

        print("\nğŸ“‹ æ–‡ç« è¯¦æƒ…:")
        for i, article in enumerate(results, 1):
            interaction = article.get('interaction_data', {})
            print(f"\n{i}. {article['title'][:60]}...")
            print(f"   ğŸ¢ è´¦å·: {article['account_name']}")
            print(f"   ğŸ“Š çƒ­åº¦: {article['hot_index']}/100")
            print(f"   ğŸ“– é˜…è¯»: {interaction.get('read_count', 0):,}")
            print(f"   ğŸ‘ ç‚¹èµ: {interaction.get('like_count', 0):,}")
            print(f"   ğŸ’ å…³é”®ä¸»é¢˜: {len(article['key_topics'])} ä¸ª")

    print(f"\nâœ¨ æŠ“å–å®Œæˆï¼Œè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {fetcher.reports_dir}")