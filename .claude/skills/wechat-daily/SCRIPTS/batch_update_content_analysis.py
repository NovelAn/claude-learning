#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ›´æ–°æ–‡ç« å†…å®¹åˆ†æ
éå† articles æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ç« ï¼Œä½¿ç”¨æ–°çš„ ContentAnalyzer è¿›è¡Œåˆ†æ
"""
import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))
sys.path.insert(0, str(current_dir.parent))

from content_analyzer import ContentAnalyzer
from config import DEEPSEEK_CONFIG


def load_article_files(articles_dir):
    """åŠ è½½æ‰€æœ‰æ–‡ç« JSONæ–‡ä»¶"""
    articles_dir = Path(articles_dir)
    article_files = list(articles_dir.glob('article-*.json'))
    return sorted(article_files)


def update_article_analysis(article_file, analyzer):
    """
    æ›´æ–°å•ç¯‡æ–‡ç« çš„å†…å®¹åˆ†æ

    Args:
        article_file: æ–‡ç« JSONæ–‡ä»¶è·¯å¾„
        analyzer: ContentAnalyzerå®ä¾‹

    Returns:
        dict: æ›´æ–°åçš„æ–‡ç« æ•°æ®
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {article_file.name}")
    print('='*80)

    # è¯»å–æ–‡ç« æ•°æ®
    with open(article_file, 'r', encoding='utf-8') as f:
        article_data = json.load(f)

    print(f"ğŸ“ æ ‡é¢˜: {article_data.get('title', 'N/A')[:60]}...")
    print(f"ğŸ¢ è´¦å·: {article_data.get('account_name', 'N/A')}")

    # æ£€æŸ¥æ˜¯å¦æœ‰ content_text
    content_text = article_data.get('content_text', '')
    if not content_text:
        print("   âš ï¸  æ–‡ç« æ²¡æœ‰ content_textï¼Œè·³è¿‡")
        return None

    print(f"   ğŸ“„ å†…å®¹é•¿åº¦: {len(content_text)} å­—ç¬¦")

    # åˆ é™¤æ—§çš„å­—æ®µ
    removed_fields = []
    if 'key_topics' in article_data:
        del article_data['key_topics']
        removed_fields.append('key_topics')

    if 'content_analysis' in article_data:
        old_analysis = article_data['content_analysis']
        # åˆ é™¤æ—§çš„åˆ†æå­—æ®µï¼Œä½†ä¿ç•™å…¶ä»–é‡è¦å­—æ®µ
        del article_data['content_analysis']
        removed_fields.append('content_analysis')

    if removed_fields:
        print(f"   ğŸ—‘ï¸  åˆ é™¤æ—§å­—æ®µ: {', '.join(removed_fields)}")

    # æ‰§è¡Œæ–°çš„å†…å®¹åˆ†æ
    print(f"\nğŸ” æ‰§è¡Œæ™ºèƒ½å†…å®¹åˆ†æ...")
    try:
        new_analysis = analyzer.analyze_article(article_data)

        # æ·»åŠ æ–°çš„åˆ†æç»“æœ
        article_data['content_analysis'] = new_analysis
        article_data['analysis_type'] = f'ai_powered_{new_analysis.get("model_used", "deepseek")}'
        article_data['analysis_updated_at'] = datetime.now().isoformat()

        # æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
        print(f"\nâœ… åˆ†æå®Œæˆ:")
        print(f"   ğŸ“ æ‘˜è¦é•¿åº¦: {len(new_analysis.get('summary', ''))} å­—ç¬¦")
        print(f"   ğŸ’¡ æ ¸å¿ƒè§‚ç‚¹: {len(new_analysis.get('key_insights', []))} æ¡")
        print(f"   ğŸ“Š å…³é”®æ•°æ®: {len(new_analysis.get('data_points', []))} ä¸ª")
        print(f"   ğŸ¢ è¯†åˆ«å®ä½“: {len(new_analysis.get('entities', []))} ä¸ª")
        print(f"   âœ… è¡ŒåŠ¨å»ºè®®: {len(new_analysis.get('recommendations', []))} æ¡")

        return article_data

    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def save_updated_article(article_data, output_file):
    """ä¿å­˜æ›´æ–°åçš„æ–‡ç« æ•°æ®"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(article_data, f, ensure_ascii=False, indent=2)
    print(f"   ğŸ’¾ å·²ä¿å­˜: {output_file}")


def batch_update_articles(articles_dir, backup=True):
    """
    æ‰¹é‡æ›´æ–°æ‰€æœ‰æ–‡ç« çš„å†…å®¹åˆ†æ

    Args:
        articles_dir: æ–‡ç« ç›®å½•è·¯å¾„
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
    """
    articles_dir = Path(articles_dir)

    if not articles_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {articles_dir}")
        return

    # åŠ è½½æ‰€æœ‰æ–‡ç« æ–‡ä»¶
    article_files = load_article_files(articles_dir)

    if not article_files:
        print(f"âŒ æœªæ‰¾åˆ°æ–‡ç« æ–‡ä»¶: {articles_dir}/article-*.json")
        return

    print(f"\nğŸ” æ‰¾åˆ° {len(article_files)} ç¯‡æ–‡ç« ")

    # åˆ›å»ºå¤‡ä»½ç›®å½•
    if backup:
        backup_dir = articles_dir.parent / 'articles_backup' / datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“¦ å¤‡ä»½ç›®å½•: {backup_dir}")

    # åˆ›å»ºå†…å®¹åˆ†æå™¨
    print(f"\nğŸ”§ åˆå§‹åŒ– ContentAnalyzer...")
    analyzer = ContentAnalyzer(
        api_key=DEEPSEEK_CONFIG.get('api_key'),
        base_url=DEEPSEEK_CONFIG.get('base_url'),
        model=DEEPSEEK_CONFIG.get('model')
    )

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(article_files),
        'success': 0,
        'failed': 0,
        'skipped': 0
    }

    # å¤„ç†æ¯ç¯‡æ–‡ç« 
    for i, article_file in enumerate(article_files, 1):
        print(f"\n[{i}/{stats['total']}] å¤„ç†ä¸­...")

        try:
            # å¤‡ä»½åŸæ–‡ä»¶
            if backup:
                import shutil
                backup_file = backup_dir / article_file.name
                shutil.copy2(article_file, backup_file)
                print(f"   ğŸ“¦ å·²å¤‡ä»½åˆ°: {backup_file.name}")

            # æ›´æ–°æ–‡ç« åˆ†æ
            updated_data = update_article_analysis(article_file, analyzer)

            if updated_data:
                # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                save_updated_article(updated_data, article_file)
                stats['success'] += 1
            else:
                stats['skipped'] += 1

        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
            stats['failed'] += 1
            continue

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ“Š æ‰¹é‡æ›´æ–°å®Œæˆç»Ÿè®¡")
    print("="*80)
    print(f"æ€»æ–‡ç« æ•°: {stats['total']}")
    print(f"âœ… æˆåŠŸæ›´æ–°: {stats['success']}")
    print(f"â­ï¸  è·³è¿‡: {stats['skipped']}")
    print(f"âŒ å¤±è´¥: {stats['failed']}")

    if backup:
        print(f"\nğŸ“¦ å¤‡ä»½ä½ç½®: {backup_dir}")

    print("\nâœ¨ æ‰¹é‡æ›´æ–°å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ğŸ”„ æ‰¹é‡æ›´æ–°æ–‡ç« å†…å®¹åˆ†æ                              â•‘
â•‘         ä½¿ç”¨æ–°çš„ DeepSeek ContentAnalyzer                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # ç¡®è®¤APIé…ç½®
    if not DEEPSEEK_CONFIG.get('api_key'):
        print("âš ï¸  æœªé…ç½®DeepSeek APIå¯†é’¥")
        print("   å°†ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼ˆåŸºç¡€è§„åˆ™åˆ†æï¼‰")
        response = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        if response != 'y':
            print("ğŸ‘‹ æ“ä½œå–æ¶ˆ")
            return
    else:
        print("âœ… DeepSeek APIå·²é…ç½®")
        cost_estimate = DEEPSEEK_CONFIG.get('max_tokens', 2000) * 0.000001  # ç²—ç•¥ä¼°ç®—
        print(f"   é¢„ä¼°æˆæœ¬: çº¦ Â¥{cost_estimate:.4f}/ç¯‡")

    # è®¾ç½®æ–‡ç« ç›®å½•
    from config import DATA_CONFIG
    articles_dir = DATA_CONFIG['articles_dir']

    print(f"\nğŸ“‚ æ–‡ç« ç›®å½•: {articles_dir}")

    # ç¡®è®¤æ“ä½œ
    print("\nâš ï¸  æ­¤æ“ä½œå°†:")
    print("   1. éå†æ‰€æœ‰æ–‡ç« æ–‡ä»¶")
    print("   2. åˆ é™¤æ—§çš„ key_topics å’Œ content_analysis å­—æ®µ")
    print("   3. ä½¿ç”¨æ–°çš„ ContentAnalyzer ç”Ÿæˆå†…å®¹åˆ†æ")
    print("   4. è‡ªåŠ¨å¤‡ä»½åŸæ–‡ä»¶")

    response = input("\næ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
    if response != 'y':
        print("ğŸ‘‹ æ“ä½œå–æ¶ˆ")
        return

    # æ‰§è¡Œæ‰¹é‡æ›´æ–°
    batch_update_articles(articles_dir, backup=True)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
