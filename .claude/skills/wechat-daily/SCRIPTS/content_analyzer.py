
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ™ºèƒ½å†…å®¹åˆ†ææ¨¡å—
ä½¿ç”¨OpenAI APIè¿›è¡Œæ·±åº¦å†…å®¹åˆ†æ
"""

import os
import json
import re
from typing import Dict, List, Any, Optional
from datetime import datetime


class ContentAnalyzer:
    """æ™ºèƒ½å†…å®¹åˆ†æå™¨ - ä½¿ç”¨OpenAI API"""

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–å†…å®¹åˆ†æå™¨

        Args:
            api_key: OpenAI APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.api_key = api_key or os.environ.get('OPENAI_API_KEY', '')
        self.model = "gpt-4o-mini"  # ä½¿ç”¨æ€§ä»·æ¯”é«˜çš„æ¨¡å‹
        self.max_tokens = 2000
        self.temperature = 0.3  # é™ä½éšæœºæ€§ï¼Œæé«˜ç¨³å®šæ€§

        if not self.api_key:
            print("âš ï¸  æœªé…ç½®OpenAI APIå¯†é’¥")
            print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY='your-key-here'")
            print("   æˆ–åœ¨ä»£ç ä¸­æä¾›api_keyå‚æ•°")

    def _call_openai_api(self, prompt: str, system_prompt: str = None) -> str:
        """
        è°ƒç”¨OpenAI API

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: APIè¿”å›çš„æ–‡æœ¬
        """
        if not self.api_key:
            return json.dumps({
                'error': 'API_KEY_MISSING',
                'message': 'æœªé…ç½®OpenAI APIå¯†é’¥'
            }, ensure_ascii=False)

        try:
            import openai

            # è®¾ç½®APIå¯†é’¥
            openai.api_key = self.api_key

            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # è°ƒç”¨API
            response = openai.OpenAI().chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )

            return response.choices[0].message.content

        except ImportError:
            return json.dumps({
                'error': 'PACKAGE_NOT_INSTALLED',
                'message': 'è¯·å…ˆå®‰è£…openaiåŒ…: pip install openai'
            }, ensure_ascii=False)
        except Exception as e:
            return json.dumps({
                'error': 'API_CALL_FAILED',
                'message': str(e)
            }, ensure_ascii=False)

    def analyze_article(self, article_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¯¹æ–‡ç« è¿›è¡Œå®Œæ•´çš„å†…å®¹åˆ†æ

        Args:
            article_data: æ–‡ç« æ•°æ®å­—å…¸ï¼ŒåŒ…å«title, content_textç­‰å­—æ®µ

        Returns:
            Dict: åˆ†æç»“æœï¼ŒåŒ…å«summary, key_insights, data_pointsç­‰
        """
        title = article_data.get('title', '')
        content = article_data.get('content_text', '')
        author = article_data.get('author', '')
        account = article_data.get('account_name', '')

        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¶…è¿‡APIé™åˆ¶
        max_content_length = 12000  # çº¦3000ä¸ªæ±‰å­—
        if len(content) > max_content_length:
            content = content[:max_content_length] + "\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"

        analysis_result = {
            'analyzer_version': '1.0.0',
            'analysis_time': datetime.now().isoformat(),
            'model_used': self.model,
            'summary': '',
            'key_insights': [],
            'data_points': [],
            'entities': [],
            'recommendations': [],
            'raw_api_response': ''
        }

        print("ğŸ” å¼€å§‹æ™ºèƒ½å†…å®¹åˆ†æ...")
        print(f"   æ ‡é¢˜: {title[:50]}...")
        print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

        # 1. ç”Ÿæˆæ–‡ç« æ‘˜è¦
        print("\nğŸ“ 1/5 ç”Ÿæˆæ–‡ç« æ‘˜è¦...")
        summary = self._generate_summary(title, content, author, account)
        analysis_result['summary'] = summary

        # 2. æå–æ ¸å¿ƒè§‚ç‚¹
        print("ğŸ’¡ 2/5 æå–æ ¸å¿ƒè§‚ç‚¹...")
        insights = self._extract_key_insights(title, content)
        analysis_result['key_insights'] = insights

        # 3. æ ‡æ³¨å…³é”®æ•°æ®
        print("ğŸ“Š 3/5 æ ‡æ³¨å…³é”®æ•°æ®...")
        data_points = self._extract_data_points(content)
        analysis_result['data_points'] = data_points

        # 4. è¯†åˆ«å®ä½“
        print("ğŸ¢ 4/5 è¯†åˆ«å“ç‰Œ/å…¬å¸/äººç‰©...")
        entities = self._extract_entities(title, content)
        analysis_result['entities'] = entities

        # 5. æç‚¼è¡ŒåŠ¨å»ºè®®
        print("âœ… 5/5 æç‚¼è¡ŒåŠ¨å»ºè®®...")
        recommendations = self._extract_recommendations(title, content)
        analysis_result['recommendations'] = recommendations

        print("\nâœ… å†…å®¹åˆ†æå®Œæˆ!")

        return analysis_result

    def _generate_summary(self, title: str, content: str, author: str, account: str) -> str:
        """
        ç”Ÿæˆæ–‡ç« æ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰

        Args:
            title: æ–‡ç« æ ‡é¢˜
            content: æ–‡ç« å†…å®¹
            author: ä½œè€…
            account: è´¦å·åç§°

        Returns:
            str: æ‘˜è¦æ–‡æœ¬
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿æ€»ç»“å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ç”Ÿæˆç®€æ˜æ‰¼è¦çš„æ–‡ç« æ‘˜è¦ï¼Œçªå‡ºæ ¸å¿ƒä¿¡æ¯å’Œä»·å€¼ç‚¹ã€‚"""

        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ˜æ‰¼è¦çš„æ‘˜è¦ï¼ˆ300å­—ä»¥å†…ï¼‰ï¼š

æ ‡é¢˜ï¼š{title}
è´¦å·ï¼š{account}
ä½œè€…ï¼š{author}

æ–‡ç« å†…å®¹ï¼š
{content}

æ‘˜è¦è¦æ±‚ï¼š
1. æ¦‚æ‹¬æ–‡ç« æ ¸å¿ƒä¸»é¢˜å’Œä¸»è¦å†…å®¹
2. çªå‡ºæœ€é‡è¦çš„ä¿¡æ¯æˆ–è§‚ç‚¹
3. è¯­è¨€ç®€æ´æ˜äº†ï¼Œæ§åˆ¶åœ¨300å­—ä»¥å†…
4. ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¡¨è¾¾

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å‰ç¼€æˆ–è¯´æ˜ï¼š"""

        response = self._call_openai_api(prompt, system_prompt)

        # æ¸…ç†å“åº”
        if response.startswith('{') and '"error"' in response:
            print(f"   âš ï¸  æ‘˜è¦ç”Ÿæˆå¤±è´¥: {response}")
            return self._generate_fallback_summary(content)

        return response.strip()

    def _generate_fallback_summary(self, content: str) -> str:
        """é™çº§æ–¹æ¡ˆï¼šåŸºäºé¦–å°¾æ®µç”Ÿæˆç®€å•æ‘˜è¦"""
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        if len(paragraphs) >= 2:
            first_part = paragraphs[0][:200]
            last_part = paragraphs[-1][:100]
            return f"{first_part}...\n\næ–‡ç« ç»“å°¾ï¼š{last_part}"
        else:
            return content[:300] + "..."

    def _extract_key_insights(self, title: str, content: str) -> List[str]:
        """
        æå–3-5ä¸ªæ ¸å¿ƒè§‚ç‚¹

        Args:
            title: æ–‡ç« æ ‡é¢˜
            content: æ–‡ç« å†…å®¹

        Returns:
            List[str]: æ ¸å¿ƒè§‚ç‚¹åˆ—è¡¨
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿æç‚¼æ–‡ç« æ ¸å¿ƒè§‚ç‚¹ã€‚
ä½ éœ€è¦ä»æ–‡ç« ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„è§‚ç‚¹æˆ–è®ºç‚¹ã€‚"""

        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡ç« ä¸­æå–3-5ä¸ªæ ¸å¿ƒè§‚ç‚¹æˆ–è®ºç‚¹ï¼š

æ ‡é¢˜ï¼š{title}

æ–‡ç« å†…å®¹ï¼š
{content}

è¦æ±‚ï¼š
1. æå–æ–‡ç« æœ€é‡è¦çš„3-5ä¸ªè§‚ç‚¹
2. æ¯ä¸ªè§‚ç‚¹ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œç®€æ´æ˜äº†
3. æŒ‰é‡è¦æ€§æ’åº
4. ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š["è§‚ç‚¹1", "è§‚ç‚¹2", "è§‚ç‚¹3"]

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼š"""

        response = self._call_openai_api(prompt, system_prompt)

        # å°è¯•è§£æJSON
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            response = response.strip()
            if response.startswith('```'):
                response = response.split('\n', 1)[1]
            if response.endswith('```'):
                response = response.rsplit('\n', 1)[0]
            response = response.strip()
            if response.startswith('json'):
                response = response[4:].strip()

            insights = json.loads(response)
            if isinstance(insights, list):
                return insights[:5]  # æœ€å¤šè¿”å›5ä¸ª
            else:
                return []
        except:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–ç¼–å·åˆ—è¡¨
            return self._parse_numbered_list(response)

    def _parse_numbered_list(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­è§£æç¼–å·åˆ—è¡¨"""
        insights = []
        lines = text.split('\n')

        for line in lines:
            line = line.strip()
            # åŒ¹é… "1." æˆ– "1ã€" æˆ– "1 )" ç­‰æ ¼å¼
            match = re.match(r'^[\d]+\s*[.ã€)]\s*(.+)', line)
            if match:
                insights.append(match.group(1).strip())
            elif len(insights) > 0 and len(insights) < 5:
                # ç»§ç»­è¡Œä½œä¸ºä¸Šä¸€ä¸ªè§‚ç‚¹çš„è¡¥å……
                if line and not line.startswith('##'):
                    insights[-1] += ' ' + line

        return insights[:5]

    def _extract_data_points(self, content: str) -> List[Dict[str, Any]]:
        """
        æå–æ–‡ç« ä¸­çš„å…³é”®æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯

        Args:
            content: æ–‡ç« å†…å®¹

        Returns:
            List[Dict]: æ•°æ®ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«value, context, category
        """
        # é¦–å…ˆä½¿ç”¨è§„åˆ™åŒ¹é…æå–æ•°å­—
        data_points = []

        # åŒ¹é…æ¨¡å¼ï¼šæ•°å­— + å•ä½/è¯´æ˜
        patterns = [
            r'([\d,]+\.?\d*)\s*([ä¸‡äº¿åƒç™¾]?\s*[å…ƒç¾é‡‘%ä¸ªç™¾åˆ†ç‚¹å€])',  # è´¢åŠ¡æ•°æ®
            r'([\d,]+)\s*([äººä¸ªæ¬¡ç¯‡ç¯‡æ¡è¯„è®º])',  # æ•°é‡ç»Ÿè®¡
            r'([\d,]+)\s*([å¹´æœˆæ—¥å­£åº¦])',  # æ—¶é—´æ•°æ®
            r'å¢é•¿\s*([\d,]+\.?\d*)\s*%?',  # å¢é•¿ç‡
            r'ä¸‹é™\s*([\d,]+\.?\d*)\s*%?',  # ä¸‹é™ç‡
        ]

        for pattern in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                value = match.group(1)
                context = self._get_context(content, match.start(), 50)

                data_points.append({
                    'value': value,
                    'unit': match.group(2) if len(match.groups()) > 1 else '',
                    'context': context,
                    'category': self._categorize_data_point(context)
                })

        # å»é‡ï¼Œé™åˆ¶æ•°é‡
        unique_points = []
        seen_values = set()
        for point in data_points[:15]:  # æœ€å¤š15ä¸ª
            value_key = point['value'] + point['unit']
            if value_key not in seen_values:
                unique_points.append(point)
                seen_values.add(value_key)

        return unique_points

    def _get_context(self, text: str, position: int, window: int = 50) -> str:
        """è·å–æ•°æ®ç‚¹çš„ä¸Šä¸‹æ–‡"""
        start = max(0, position - window)
        end = min(len(text), position + window)
        return text[start:end].strip()

    def _categorize_data_point(self, context: str) -> str:
        """æ ¹æ®ä¸Šä¸‹æ–‡å¯¹æ•°æ®ç‚¹åˆ†ç±»"""
        if any(word in context for word in ['å…ƒ', 'è¥æ”¶', 'åˆ©æ¶¦', 'é”€å”®é¢', 'å¸‚å€¼']):
            return 'è´¢åŠ¡æ•°æ®'
        elif any(word in context for word in ['%', 'å¢é•¿', 'ä¸‹é™', 'ç‡']):
            return 'ç™¾åˆ†æ¯”/æ¯”ç‡'
        elif any(word in context for word in ['å¹´', 'å­£åº¦', 'æœˆ']):
            return 'æ—¶é—´æ•°æ®'
        else:
            return 'å…¶ä»–æ•°æ®'

    def _extract_entities(self, title: str, content: str) -> List[Dict[str, str]]:
        """
        è¯†åˆ«æ–‡ç« ä¸­çš„å“ç‰Œã€å…¬å¸ã€äººç‰©ç­‰å®ä½“

        Args:
            title: æ–‡ç« æ ‡é¢˜
            content: æ–‡ç« å†…å®¹

        Returns:
            List[Dict]: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«name, type, description
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å®ä½“è¯†åˆ«ä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«æ–‡ç« ä¸­çš„å“ç‰Œã€å…¬å¸ã€äººç‰©ç­‰å®ä½“ã€‚"""

        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡ç« ä¸­è¯†åˆ«é‡è¦çš„å“ç‰Œã€å…¬å¸ã€äººç‰©å®ä½“ï¼š

æ ‡é¢˜ï¼š{title}

æ–‡ç« å†…å®¹ï¼š
{content}

è¦æ±‚ï¼š
1. è¯†åˆ«æ–‡ç« ä¸­æåˆ°çš„å“ç‰Œã€å…¬å¸ã€äººç‰©
2. è¿”å›JSONæ•°ç»„æ ¼å¼ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameï¼ˆåç§°ï¼‰ã€typeï¼ˆç±»å‹ï¼šå“ç‰Œ/å…¬å¸/äººç‰©ï¼‰ã€descriptionï¼ˆç®€è¦æè¿°ï¼‰
3. æœ€å¤šè¿”å›10ä¸ªæœ€é‡è¦çš„å®ä½“
4. ç¤ºä¾‹æ ¼å¼ï¼š[{{"name": "LVMH", "type": "å…¬å¸", "description": "å…¨çƒæœ€å¤§çš„å¥¢ä¾ˆå“é›†å›¢"}}]

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼š"""

        response = self._call_openai_api(prompt, system_prompt)

        # è§£æJSONå“åº”
        try:
            # æ¸…ç†markdownæ ‡è®°
            response = response.strip()
            if response.startswith('```'):
                response = response.split('\n', 1)[1]
            if response.endswith('```'):
                response = response.rsplit('\n', 1)[0]
            response = response.strip()
            if response.startswith('json'):
                response = response[4:].strip()

            entities = json.loads(response)
            if isinstance(entities, list):
                return entities[:10]
            else:
                return []
        except:
            # é™çº§ï¼šä½¿ç”¨å…³é”®è¯åŒ¹é…
            return self._extract_entities_fallback(title, content)

    def _extract_entities_fallback(self, title: str, content: str) -> List[Dict[str, str]]:
        """é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨å·²çŸ¥å®ä½“åº“åŒ¹é…"""
        entities = []
        text = title + ' ' + content

        # å·²çŸ¥å“ç‰Œ/å…¬å¸åº“ï¼ˆå¥¢ä¾ˆå“/æ—¶å°šè¡Œä¸šï¼‰
        known_brands = {
            'LVMH': 'å…¬å¸',
            'Gucci': 'å“ç‰Œ',
            'Prada': 'å“ç‰Œ',
            'Ferragamo': 'å“ç‰Œ',
            'Cartier': 'å“ç‰Œ',
            'HermÃ¨s': 'å“ç‰Œ',
            'Chanel': 'å“ç‰Œ',
            'Dior': 'å“ç‰Œ',
            'Louis Vuitton': 'å“ç‰Œ',
            'Burberry': 'å“ç‰Œ',
            'Tiffany': 'å“ç‰Œ',
            'Coach': 'å“ç‰Œ',
            'Nike': 'å“ç‰Œ',
            'Adidas': 'å“ç‰Œ',
            'Lululemon': 'å“ç‰Œ',
            'Abercrombie': 'å“ç‰Œ',
        }

        for brand, brand_type in known_brands.items():
            if brand in text:
                entities.append({
                    'name': brand,
                    'type': brand_type,
                    'description': f'æ£€æµ‹åˆ°{brand}ç›¸å…³å†…å®¹'
                })

        return entities[:10]

    def _extract_recommendations(self, title: str, content: str) -> List[str]:
        """
        æç‚¼æ–‡ç« ä¸­çš„è¡ŒåŠ¨å»ºè®®æˆ–å¯ç¤º

        Args:
            title: æ–‡ç« æ ‡é¢˜
            content: æ–‡ç« å†…å®¹

        Returns:
            List[str]: è¡ŒåŠ¨å»ºè®®åˆ—è¡¨
        """
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»æ–‡ç« ä¸­æç‚¼å¯æ“ä½œçš„å»ºè®®å’Œå¯ç¤ºã€‚"""

        prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡ç« ä¸­æç‚¼å¯æ“ä½œçš„è¡ŒåŠ¨å»ºè®®æˆ–è¡Œä¸šå¯ç¤ºï¼š

æ ‡é¢˜ï¼š{title}

æ–‡ç« å†…å®¹ï¼š
{content}

è¦æ±‚ï¼š
1. æç‚¼3-5æ¡å¯æ“ä½œçš„å»ºè®®æˆ–å¯ç¤º
2. å»ºè®®åº”è¯¥å…·ä½“ã€å®ç”¨ï¼Œèƒ½å¤ŸæŒ‡å¯¼å®é™…å·¥ä½œ
3. ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š["å»ºè®®1", "å»ºè®®2", "å»ºè®®3"]
4. å¦‚æœæ–‡ç« æ²¡æœ‰æ˜ç¡®çš„å»ºè®®ï¼Œè¯·æç‚¼å‡ºå¯¹è¡Œä¸šçš„å¯ç¤ºæˆ–è§‚å¯Ÿ

è¯·ç›´æ¥è¿”å›JSONæ•°ç»„ï¼š"""

        response = self._call_openai_api(prompt, system_prompt)

        # è§£æJSON
        try:
            # æ¸…ç†markdownæ ‡è®°
            response = response.strip()
            if response.startswith('```'):
                response = response.split('\n', 1)[1]
            if response.endswith('```'):
                response = response.rsplit('\n', 1)[0]
            response = response.strip()
            if response.startswith('json'):
                response = response[4:].strip()

            recommendations = json.loads(response)
            if isinstance(recommendations, list):
                return recommendations[:5]
            else:
                return []
        except:
            # JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–ç¼–å·åˆ—è¡¨
            return self._parse_numbered_list(response)[:5]

    def format_analysis_report(self, analysis: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå¯è¯»æŠ¥å‘Š

        Args:
            analysis: åˆ†æç»“æœå­—å…¸

        Returns:
            str: æ ¼å¼åŒ–æŠ¥å‘Š
        """
        report = []
        report.append("=" * 80)
        report.append("ğŸ“Š æ™ºèƒ½å†…å®¹åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)

        # æ‘˜è¦
        if analysis.get('summary'):
            report.append("\nğŸ“ æ–‡ç« æ‘˜è¦ï¼š")
            report.append("-" * 80)
            report.append(analysis['summary'])

        # æ ¸å¿ƒè§‚ç‚¹
        if analysis.get('key_insights'):
            report.append("\nğŸ’¡ æ ¸å¿ƒè§‚ç‚¹ï¼š")
            report.append("-" * 80)
            for i, insight in enumerate(analysis['key_insights'], 1):
                report.append(f"{i}. {insight}")

        # å…³é”®æ•°æ®
        if analysis.get('data_points'):
            report.append("\nğŸ“Š å…³é”®æ•°æ®ï¼š")
            report.append("-" * 80)
            for i, point in enumerate(analysis['data_points'][:10], 1):
                report.append(f"{i}. {point['value']} {point['unit']} ({point['category']})")
                report.append(f"   ä¸Šä¸‹æ–‡: {point['context'][:60]}...")

        # å®ä½“è¯†åˆ«
        if analysis.get('entities'):
            report.append("\nğŸ¢ è¯†åˆ«å®ä½“ï¼š")
            report.append("-" * 80)
            for entity in analysis['entities']:
                report.append(f"â€¢ {entity['name']} ({entity['type']})")
                if entity.get('description'):
                    report.append(f"  {entity['description']}")

        # è¡ŒåŠ¨å»ºè®®
        if analysis.get('recommendations'):
            report.append("\nâœ… è¡ŒåŠ¨å»ºè®®ï¼š")
            report.append("-" * 80)
            for i, rec in enumerate(analysis['recommendations'], 1):
                report.append(f"{i}. {rec}")

        report.append("\n" + "=" * 80)
        report.append(f"åˆ†ææ—¶é—´: {analysis.get('analysis_time', 'N/A')}")
        report.append(f"ä½¿ç”¨æ¨¡å‹: {analysis.get('model_used', 'N/A')}")

        return '\n'.join(report)


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # æµ‹è¯•å†…å®¹åˆ†æå™¨
    analyzer = ContentAnalyzer()

    # æ¨¡æ‹Ÿæ–‡ç« æ•°æ®
    test_article = {
        'title': 'çªå‘ | Ferragamoä¸ä¸­å›½é•¿æœŸä¼™ä¼´è‚¡ä¸œåè®®åˆ°æœŸä¸ç»­',
        'author': 'Drizzie',
        'account_name': 'æ—¶å°šå•†ä¸šDaily',
        'content_text': '''
        æ„å¤§åˆ©å¥¢ä¾ˆå“ç‰Œ Salvatore Ferragamoï¼ˆè²æ‹‰æ ¼æ…•ï¼‰å®£å¸ƒä¸ä¸­å›½é•¿æœŸåˆä½œä¼™ä¼´çš„è‚¡ä¸œåè®®å·²åˆ°æœŸï¼Œå°†ä¸å†ç»­ç­¾ã€‚
        è¿™ä¸€å†³å®šæ ‡å¿—ç€è¯¥å“ç‰Œåœ¨ä¸­å›½å¸‚åœºæˆ˜ç•¥çš„é‡å¤§è°ƒæ•´ã€‚

        æ®äº†è§£ï¼ŒFerragamoä¸è¯¥åˆä½œä¼™ä¼´çš„åˆä½œè¶…è¿‡10å¹´ï¼ŒæœŸé—´å…±åŒå¼€æ‹“äº†ä¸­å›½å¸‚åœºã€‚
        åˆ†æè®¤ä¸ºï¼Œæ­¤æ¬¡è°ƒæ•´å¯èƒ½ä¸å“ç‰Œå…¨çƒæˆ˜ç•¥è½¬å‹æœ‰å…³ã€‚

        æ•°æ®æ˜¾ç¤ºï¼ŒFerragamo 2023å¹´åœ¨ä¸­å›½å¸‚åœºçš„é”€å”®é¢è¾¾åˆ°2.3äº¿æ¬§å…ƒï¼Œå å…¨çƒé”€å”®é¢çš„15%ã€‚
        ç„¶è€Œï¼Œè¿‘å¹´æ¥ä¸­å›½å¥¢ä¾ˆå“å¸‚åœºå¢é€Ÿæ”¾ç¼“ï¼Œ2023å¹´å¢é•¿ç‡ä»…ä¸º12%ï¼Œä½äºå‰å¹´çš„28%ã€‚

        ä¸šå†…äººå£«æŒ‡å‡ºï¼ŒFerragamoæ­£åœ¨é‡æ–°è¯„ä¼°å…¶åœ¨ä¸­å›½å¸‚åœºçš„åˆ†é”€ç­–ç•¥ï¼Œå¯èƒ½è½¬å‘ç›´è¥æ¨¡å¼ã€‚
        è¿™ä¸€è¶‹åŠ¿åœ¨å¥¢ä¾ˆå“è¡Œä¸šå¹¶ä¸é²œè§ï¼ŒGucciã€Pradaç­‰å“ç‰Œéƒ½åœ¨åŠ å¼ºç›´è¥æ¸ é“å»ºè®¾ã€‚
        '''
    }

    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½å†…å®¹åˆ†æåŠŸèƒ½...\n")

    # æ‰§è¡Œåˆ†æ
    result = analyzer.analyze_article(test_article)

    # æ‰“å°æŠ¥å‘Š
    print("\n" + analyzer.format_analysis_report(result))

    # ä¿å­˜ç»“æœ
    output_file = 'test_content_analysis.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
