#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆç¾è§‚çš„ç½‘é¡µç‰ˆäº¤äº’åˆ†ææŠ¥å‘Š
"""

import json
import os
import re
from datetime import datetime
from typing import Dict, List, Optional
import jinja2

class WeChatReportGenerator:
    """å¾®ä¿¡å…¬ä¼—å·æ•°æ®åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, template_dir: Optional[str] = None):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        self.reports_dir = DATA_CONFIG['reports_dir']
        os.makedirs(self.reports_dir, exist_ok=True)

        # è®¾ç½®Jinja2æ¨¡æ¿ç¯å¢ƒ
        template_dir = template_dir or os.path.join(os.path.dirname(os.path.abspath(__file__)), '../templates')
        os.makedirs(template_dir, exist_ok=True)
        self.template_env = jinja2.Environment(loader=jinja2.FileSystemLoader(template_dir))

    def generate_weekly_report(self, articles_data: List[Dict], output_file: Optional[str] = None) -> str:
        """
        ç”Ÿæˆå‘¨æŠ¥

        Args:
            articles_data: åŒ…å«æ‰€æœ‰æ–‡ç« æ•°æ®çš„åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰

        Returns:
            ç”Ÿæˆçš„HTMLæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if not articles_data:
            print("âš ï¸  æ²¡æœ‰æ–‡ç« æ•°æ®å¯ç”¨äºç”ŸæˆæŠ¥å‘Š")
            return ""

        print("ğŸ¯ æ­£åœ¨ç”Ÿæˆå…¬ä¼—å·çƒ­ç‚¹åˆ†æå‘¨æŠ¥...")

        # åˆ†ææ•°æ®
        analysis = self._analyze_batch_data(articles_data)

        # æ¸²æŸ“æ¨¡æ¿
        template = self.template_env.get_template('report.html')
        html_content = template.render(
            report_title="å¾®ä¿¡å…¬ä¼—å·çƒ­ç‚¹åˆ†æå‘¨æŠ¥",
            report_date=datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥"),
            **analysis
        )

        # ä¿å­˜HTMLæŠ¥å‘Š
        if not output_file:
            output_file = f"weekly-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.html"
        output_path = os.path.join(self.reports_dir, output_file)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"ğŸ‰ å‘¨æŠ¥å·²ç”Ÿæˆ: {output_path}")
        return output_path

    def _analyze_batch_data(self, articles_data: List[Dict]) -> Dict:
        """åˆ†ææ‰¹é‡æ–‡ç« æ•°æ®"""
        # åŸºç¡€ç»Ÿè®¡
        total_articles = len(articles_data)

        # åªå¤„ç†æœ‰äº’åŠ¨æ•°æ®çš„æ–‡ç« 
        articles_with_data = [a for a in articles_data if a.get('interaction_data')]

        if not articles_with_data:
            return self._create_empty_report()

        # è®¡ç®—æ€»æŒ‡æ ‡
        total_reads = int(sum(a['interaction_data'].get('read_count', 0) for a in articles_with_data) / 10000)  # è½¬æ¢ä¸ºä¸‡
        total_likes = sum(a['interaction_data'].get('like_count', 0) for a in articles_with_data)
        total_shares = sum(a['interaction_data'].get('share_count', 0) for a in articles_with_data)
        total_comments = sum(a['interaction_data'].get('comment_count', 0) for a in articles_with_data)
        total_collects = sum(a['interaction_data'].get('collect_count', 0) for a in articles_with_data)  # æ·»åŠ æ”¶è—æ€»æ•°

        # è®¡ç®—å¹³å‡æ•°æ®
        avg_like_rate = (total_likes / (total_reads * 10000) * 100) if total_reads > 0 else 0
        avg_share_rate = (total_shares / (total_reads * 10000) * 100) if total_reads > 0 else 0
        avg_comment_rate = (total_comments / (total_reads * 10000) * 100) if total_reads > 0 else 0
        avg_collect_rate = (total_collects / (total_reads * 10000) * 100) if total_reads > 0 else 0  # æ·»åŠ å¹³å‡æ”¶è—ç‡
        top_hot_index = max(a.get('hot_index', 0) for a in articles_with_data)

        # çƒ­é—¨æ–‡ç« æ’åº
        sorted_articles = sorted(articles_with_data, key=lambda x: x.get('hot_index', 0), reverse=True)[:10]

        # å…³é”®è¯çƒ­åº¦åˆ†æ
        all_topics = []
        for article in articles_with_data:
            topics = article.get('content_analysis', {}).get('key_topics', [])
            for topic in topics[:5]:  # æ¯ç¯‡æ–‡ç« å–å‰5ä¸ªä¸»é¢˜
                all_topics.append({
                    'name': topic.get('term', ''),
                    'score': topic.get('weight', 0),
                    'article': article['title']
                })

        # èšåˆä¸»é¢˜å¾—åˆ†
        topic_scores = {}
        for topic in all_topics:
            if topic['name'] and len(topic['name']) >= 2:
                topic_scores[topic['name']] = topic_scores.get(topic['name'], 0) + topic['score']

        top_topics = sorted(
            [{'name': name, 'score': round(score, 2)} for name, score in topic_scores.items()],
            key=lambda x: x['score'],
            reverse=True
        )[:20]

        return {
            'total_articles': total_articles,
            'articles_with_data': len(articles_with_data),
            'total_reads': f"{total_reads}",
            'total_likes': f"{total_likes}",
            'avg_like_rate': f"{avg_like_rate:.2f}%",
            'avg_share_rate': f"{avg_share_rate:.2f}%",
            'avg_comment_rate': f"{avg_comment_rate:.2f}%",
            'top_hot_index': int(top_hot_index),
            'hot_articles': [
                {
                    'title': a['title'],
                    'account_name': a['account_name'],
                    'read_count': f"{int(a['interaction_data'].get('read_count', 0) / 10000 * 10000):,}",
                    'like_count': f"{a['interaction_data'].get('like_count', 0)}",
                    'comment_count': f"{a['interaction_data'].get('comment_count', 0)}",
                    'collect_count': f"{a['interaction_data'].get('collect_count', 0)}",  # æ·»åŠ æ”¶è—æ•°
                    'hot_index': int(a.get('hot_index', 0))
                }
                for a in sorted_articles
            ],
            'top_topics': top_topics[:15],
            'content_analysis': self._generate_content_analysis(articles_with_data),
            'interaction_analysis': self._generate_interaction_analysis(articles_with_data),
            'insights': self._generate_insights(articles_with_data, {
                'total_reads': total_reads,
                'total_likes': total_likes,
                'avg_like_rate': avg_like_rate
            }),
            'generated_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'api_credits_used': len(articles_with_data)
        }

    def _create_empty_report(self) -> Dict:
        """åˆ›å»ºç©ºæŠ¥å‘Š"""
        return {
            'total_articles': 0,
            'total_reads': "0",
            'total_likes': "0",
            'avg_like_rate': "0.00%",
            'top_hot_index': 0,
            'hot_articles': [],
            'top_topics': [],
            'content_analysis': 'æš‚æ— å†…å®¹å¯åˆ†æ',
            'interaction_analysis': 'æš‚æ— äº’åŠ¨æ•°æ®',
            'insights': ['è¯·æä¾›æœ‰æ•ˆçš„æ–‡ç« æ•°æ®åé‡æ–°ç”ŸæˆæŠ¥å‘Š'],
            'generated_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'api_credits_used': 0
        }

    def _generate_content_analysis(self, articles: List[Dict]) -> str:
        """ç”Ÿæˆå†…å®¹åˆ†æéƒ¨åˆ†"""
        metrics = []

        # å†…å®¹é•¿åº¦åˆ†æ
        lengths = [len(a.get('content_text', '')) for a in articles]
        if lengths:
            avg_length = sum(lengths) / len(lengths)
            if avg_length > 3000:
                metrics.append("æ–‡ç« æ™®éè¾ƒé•¿ï¼Œå†…å®¹æ·±åº¦è¾ƒé«˜")
            elif avg_length > 1500:
                metrics.append("æ–‡ç« é•¿åº¦é€‚ä¸­ï¼Œå¹³è¡¡äº†ä¿¡æ¯å¯†åº¦å’Œå¯è¯»æ€§")
            else:
                metrics.append("æ–‡ç« å†…å®¹ç›¸å¯¹ç®€æ´ï¼Œæ³¨é‡å¿«é€Ÿä¼ è¾¾è¦ç‚¹")

        # æƒ…ç»ªåˆ†æ
        tones = [a.get('tone_analysis', 'neutral') for a in articles]
        positive_pct = (tones.count('positive') / len(tones)) * 100
        negative_pct = (tones.count('negative') / len(tones)) * 100

        if positive_pct > 60:
            metrics.append(f"æ•´ä½“æƒ…ç»ªç§¯ææ­£é¢ï¼ˆ{positive_pct:.0f}%çš„æ–‡ç« è¡¨ç°ä¹è§‚æ€åº¦ï¼‰")
        elif negative_pct > 60:
            metrics.append(f"æ•´ä½“æƒ…ç»ªåè°¨æ…ï¼Œ{negative_pct:.0f}%çš„æ–‡ç« å‘ˆç°è´Ÿé¢å€¾å‘")
        else:
            metrics.append("å†…å®¹æƒ…ç»ªä¸­æ€§ï¼Œä»¥å®¢è§‚æŠ¥é“å’Œåˆ†æä¸ºä¸»")

        return "\n".join(f"â€¢ {metric}" for metric in metrics)

    def _generate_interaction_analysis(self, articles: List[Dict]) -> str:
        """ç”Ÿæˆäº’åŠ¨åˆ†æ"""
        metrics = []

        # äº’åŠ¨æ•°æ®èšåˆ
        total_reads = sum(a['interaction_data'].get('read_count', 0) for a in articles)
        total_likes = sum(a['interaction_data'].get('like_count', 0) for a in articles)
        total_comments = sum(a['interaction_data'].get('comment_count', 0) for a in articles)

        avg_like_rate = (total_likes / max(total_reads, 1) * 100)
        avg_comment_rate = (total_comments / max(total_reads, 1) * 100)

        # åŸºå‡†å¯¹æ¯”
        if avg_like_rate >= 3:
            metrics.append(f"ç‚¹èµç‡ {avg_like_rate:.2f}% é«˜äºè¡Œä¸šå¹³å‡æ°´å¹³ï¼ˆ2-3%ï¼‰")
        elif avg_like_rate >= 2:
            metrics.append(f"ç‚¹èµç‡ {avg_like_rate:.2f}% å¤„äºè¡Œä¸šå¹³å‡æ°´å¹³")
        else:
            metrics.append(f"ç‚¹èµç‡ {avg_like_rate:.2f}% ä½äºè¡Œä¸šå¹³å‡ï¼Œéœ€ä¼˜åŒ–å†…å®¹å¸å¼•åŠ›")

        # æ·±åº¦å‚ä¸åˆ†æ
        if avg_comment_rate >= 0.8:
            metrics.append("è¯»è€…è¯„è®ºå‚ä¸åº¦é«˜ï¼Œå†…å®¹èƒ½å¤Ÿå¼•å‘æ·±åº¦æ€è€ƒå’Œè®¨è®º")
        elif avg_comment_rate >= 0.5:
            metrics.append("è¯„è®ºå‚ä¸åº¦è‰¯å¥½ï¼Œæœ‰ä¸€å®šè¯é¢˜è®¨è®ºåŸºç¡€")
        else:
            metrics.append("è¯„è®ºå‚ä¸åº¦åä½ï¼Œå»ºè®®å¢åŠ äº’åŠ¨å¼•å¯¼å…ƒç´ ")

        # ä¸åŒç±»å‹æ–‡ç« è¡¨ç°
        high_engagement = [a for a in articles if a.get('hot_index', 0) >= 70]
        if len(high_engagement) > len(articles) / 3:
            metrics.append(f"æœ‰ {len(high_engagement)} ç¯‡æ–‡ç« è·å¾—é«˜äº’åŠ¨ï¼Œé€‰é¢˜ç­–ç•¥æˆåŠŸ")

        return "\n".join(f"â€¢ {metric}" for metric in metrics)

    def _generate_insights(self, articles: List[Dict], metrics: Dict) -> List[str]:
        """ç”Ÿæˆæ´å¯Ÿå»ºè®®"""
        insights = []

        # é˜…è¯»é‡æ´å¯Ÿ
        read_millions = metrics['total_reads']
        if read_millions >= 100:
            insights.append(f"æ€»é˜…è¯»é‡çªç ´{read_millions:.0f}ä¸‡ï¼Œè¾¾åˆ°çˆ†æ¬¾é‡çº§ï¼Œå»ºè®®è¶çƒ­æ‰“é“æŒç»­æ¨å‡ºç›¸å…³å†…å®¹")
        elif read_millions >= 50:
            insights.append(f"ç´¯è®¡é˜…è¯»é‡è¾¾åˆ°{read_millions:.0f}ä¸‡ï¼Œåœ¨è¯¥å‚ç›´é¢†åŸŸè¡¨ç°äº®çœ¼ï¼Œå¯è€ƒè™‘æ‰©å¤§é€‰é¢˜èŒƒå›´")

        # äº’åŠ¨è¡¨ç°
        like_rate = metrics['avg_like_rate']
        if like_rate >= 5:
            insights.append("ç‚¹èµç‡è¡¨ç°ä¼˜å¼‚ï¼Œå†…å®¹è´¨é‡å¾—åˆ°äº†è¯»è€…çš„é«˜åº¦è®¤å¯ï¼Œå»ºè®®æ€»ç»“æˆåŠŸè¦ç´ æŒç»­å¤ç”¨")
        elif like_rate >= 3:
            insights.append("äº’åŠ¨è¡¨ç°è‰¯å¥½ï¼Œå·²å½¢æˆç¨³å®šçš„è¯»è€…å‚ä¸åŸºç¡€ï¼Œå¯è€ƒè™‘å¼•å¯¼æ›´å¤šåˆ†äº«å’Œè®¨è®º")
        else:
            insights.append("äº’åŠ¨å‚ä¸æœ‰å¾…æå‡ï¼Œå»ºè®®åœ¨æ–‡ç« ç»“å°¾å¢åŠ å¼•å¯¼ç‚¹èµæˆ–è¯„è®ºçš„è¡ŒåŠ¨å·å¬")

        # çƒ­é—¨è¯é¢˜æ´å¯Ÿ
        top_topics = list(set([topic.get('term', '') for a in articles
                              for topic in a.get('content_analysis', {}).get('key_topics', [])[:3]]))
        if len(top_topics) >= 5:
            insights.append(f"æœ¬å‘¨çƒ­é—¨è®¨è®ºè¯é¢˜åŒ…æ‹¬ï¼š{', '.join(top_topics[:3])}ï¼Œè¿™äº›ä¸»é¢˜è·å¾—äº†è¾ƒé«˜å…³æ³¨")

        # æˆåŠŸæ–‡ç« åˆ†æ
        best_article = max(articles, key=lambda x: x.get('hot_index', 0))
        best_score = best_article.get('hot_index', 0)
        if best_score >= 90:
            insights.append(f"ã€Œ{best_article['title'][:30]}...ã€çƒ­åº¦æŒ‡æ•°é«˜è¾¾{best_score}ï¼Œå¯æ·±åº¦åˆ†æå…¶æˆåŠŸè¦ç´ ç”¨äºåç»­é€‰é¢˜")

        # è´¦å·è¡¨ç°
        accounts = list(set(a['account_name'] for a in articles if a['account_name'] != 'æœªçŸ¥å…¬ä¼—å·'))
        if len(accounts) >= 3:
            insights.append(f"æœ¬å‘¨åˆ†æäº†{len(accounts)}ä¸ªä¸åŒè´¦å·çš„å†…å®¹ï¼Œå¤šå…ƒåŒ–è§†è§’å¸®åŠ©å‘ç°æ›´å¤šçƒ­é—¨é€‰é¢˜æœºä¼š")

        return insights[:6]  # æœ€å¤š6æ¡æ´å¯Ÿ

if __name__ == '__main__':
    generator = WeChatReportGenerator()

    # æ¼”ç¤ºç”¨çš„ç¤ºä¾‹æ•°æ®
    demo_data = {
        'title': 'çªå‘ | Ferragamoä¸ä¸­å›½é•¿æœŸä¼™ä¼´è‚¡ä¸œåè®®åˆ°æœŸä¸ç»­',
        'account_name': 'æ—¶å°šå•†ä¸šDaily',
        'author': 'Drizzie',
        'publish_time': '2025-12-30',
        'content_text': 'è¿™æ˜¯ç¤ºä¾‹æ–‡ç« å†…å®¹ï¼ŒåŒ…å«äº†è¯¦ç»†çš„åˆ†æ...',
        'hot_index': 85,
        'tone_analysis': 'negative',
        'key_topics': [{'term': 'å¥¢ä¾ˆå“', 'weight': 0.8}, {'term': 'è‚¡ä¸œåè®®', 'weight': 0.7}],
        'interaction_data': {
            'read_count': 50212,
            'like_count': 1094,
            'like_rate': 0.0218,
            'comment_count': 293
        }
    }

    print("ğŸ“‹ ç”Ÿæˆç¤ºä¾‹æŠ¥å‘Š...")
    report_path = generator.generate_weekly_report([demo_data])
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")