#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WeChat Data Synchronization Module
/expression>å°†äº’åŠ¨æ•°æ®è·å–åŠŸèƒ½é›†æˆåˆ°/wechat-daily Skillå·¥ä½œæµä¸­
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional

# å¯¼å…¥äº’åŠ¨æ•°æ®è·å–æ¨¡å—
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from fetch_interaction_data import WeChatInteractionFetcher as Fetcher

class WeChatDataSync:
    """æ•´åˆæ•°æ®è·å–ã€åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆçš„å®Œæ•´å·¥ä½œæµ"""

    def __init__(self, config_path: str = "config.json"):
        """
        åˆå§‹åŒ–æ•°æ®åŒæ­¥å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«APIå¯†é’¥ç­‰ä¿¡æ¯
        """
        self.config = self._load_config(config_path)
        self.interaction_fetcher = Fetcher(
            self.config.get('interaction_api_key')
        )

        # åŸºç¡€è·¯å¾„è®¾ç½®
        self.data_dir = "data"
        self.articles_dir = os.path.join(self.data_dir, "articles")
        self.reports_dir = os.path.join(self.data_dir, "reports")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for dir_path in [self.articles_dir, self.reports_dir]:
            os.makedirs(dir_path, exist_ok=True)

    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                print(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return {}
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ï¼š{e}")
            return {}

    def sync_article_with_interaction(self, article_file: str) -> Optional[Dict]:
        """
        ä¸ºå·²æœ‰æ–‡ç« æ·»åŠ äº’åŠ¨æ•°æ®

        Args:
            article_file: æ–‡ç« JSONæ–‡ä»¶è·¯å¾„

        Returns:
            æ›´æ–°åçš„æ–‡ç« æ•°æ®
        """
        try:
            # è¯»å–æ–‡ç« åŸºç¡€æ•°æ®
            with open(article_file, 'r', encoding='utf-8') as f:
                article_data = json.load(f)

            print(f"æ­£åœ¨åŒæ­¥æ–‡ç« äº’åŠ¨æ•°æ®: {article_data.get('title', 'æœªçŸ¥æ ‡é¢˜')}")

            # è·å–äº’åŠ¨æ•°æ®
            interaction_data = self.interaction_fetcher.get_interaction_data(
                article_data['url']
            )

            if interaction_data:
                # åˆå¹¶æ•°æ®
                article_data['interaction_metrics'] = interaction_data

                # è·å–é¢å¤–çš„è´¨é‡åˆ†æ
                quality_analysis = self.interaction_fetcher.analyze_interaction_quality(
                    interaction_data
                )
                article_data['interaction_metrics']['quality_analysis'] = quality_analysis

                # è®¡ç®—çƒ­åº¦æŒ‡æ•°
                hot_index = self._calculate_hot_index(interaction_data, article_data)
                article_data['hot_index'] = hot_index

                # æ›´æ–°åŒæ­¥æ—¶é—´
                article_data['interaction_synced_at'] = datetime.now().isoformat()

                # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                self._save_updated_article(article_file, article_data)

                print(f"âœ… æ•°æ®åŒæ­¥å®Œæˆ")
                print(f"   é˜…è¯»æ•°: {interaction_data['read_count']:,}")
                print(f"   ç‚¹èµæ•°: {interaction_data['like_count']:,}")
                print(f"   çƒ­åº¦æŒ‡æ•°: {hot_index}/100")

                return article_data
            else:
                print("âš ï¸  æœªèƒ½è·å–äº’åŠ¨æ•°æ®")
                return None

        except Exception as e:
            print(f"åŒæ­¥å¤±è´¥ï¼š{e}")
            return None

    def _calculate_hot_index(self, interaction_data: Dict, article_data: Dict) -> int:
        """
        è®¡ç®—æ–‡ç« çƒ­åº¦æŒ‡æ•°

        Args:
            interaction_data: äº’åŠ¨æ•°æ®
            article_data: æ–‡ç« å†…å®¹æ•°æ®

        Returns:
            çƒ­åº¦æŒ‡æ•°ï¼ˆ0-100ï¼‰
        """
        score = 0

        # 1. é˜…è¯»æ•°æƒé‡ï¼ˆ50åˆ†ï¼‰
        read_count = interaction_data.get('read_count', 0)
        if read_count >= 100000:
            score += 50
        elif read_count >= 50000:
            score += 40
        elif read_count >= 10000:
            score += 30
        elif read_count >= 5000:
            score += 20
        elif read_count >= 1000:
            score += 10

        # 2. äº’åŠ¨ç‡æƒé‡ï¼ˆ30åˆ†ï¼‰
        like_count = interaction_data.get('like_count', 0)
        if read_count > 0:
            like_rate = like_count / read_count * 100

            if like_rate >= 5:
                score += 30
            elif like_rate >= 3:
                score += 25
            elif like_rate >= 2:
                score += 20
            elif like_rate >= 1:
                score += 15
            else:
                score += 10

        # 3. å†…å®¹ä»·å€¼æƒé‡ï¼ˆ15åˆ†ï¼‰
        content_length = len(article_data.get('content_text', ''))
        key_topics = len(article_data.get('content_analysis', {}).get('key_topics', []))

        if content_length >= 2000:
            score += 5
        if key_topics >= 10:
            score += 5
        if article_data.get('account_name') in ['æ—¶å°šå•†ä¸šDaily', 'HYPEBEAST']:
            score += 5  # æƒå¨æ€§è´¦å·åŠ åˆ†

        # 4. æ—¶æ•ˆæ€§æƒé‡ï¼ˆ5åˆ†ï¼‰
        publish_time = article_data.get('publish_time')
        if publish_time:
            days_old = (datetime.now() - datetime.fromisoformat(publish_time)).days
            if days_old <= 1:
                score += 5
            elif days_old <= 3:
                score += 3
            elif days_old <= 7:
                score += 1

        return min(score, 100)

    def _save_updated_article(self, article_path: str, data: Dict):
        """ä¿å­˜æ›´æ–°åçš„æ–‡ç« æ•°æ®"""
        # åœ¨åŸæ–‡ä»¶ååŸºç¡€ä¸Šæ·»åŠ æ›´æ–°æ—¶é—´æˆ³
        base_name = os.path.basename(article_path)
        name_parts = base_name.split('.')
        updated_name = f"{name_parts[0]}-with-interaction.{name_parts[1]}"

        save_path = os.path.join(os.path.dirname(article_path), updated_name)

        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def sync_all_articles(self) -> List[Dict]:
        """æ‰¹é‡åŒæ­¥æ‰€æœ‰å·²æœ‰æ–‡ç« """
        print("ğŸ”„ å¼€å§‹æ‰¹é‡åŒæ­¥æ–‡ç« äº’åŠ¨æ•°æ®...")

        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« æ–‡ä»¶
        article_files = []
        for file in os.listdir(self.articles_dir):
            if file.startswith('article-') and file.endswith('.json') and 'with-interaction' not in file:
                article_files.append(os.path.join(self.articles_dir, file))

        if not article_files:
            print("âš ï¸  æœªæ‰¾åˆ°éœ€è¦åŒæ­¥çš„æ–‡ç« ")
            return []

        print(f"ğŸ“‹ å‘ç° {len(article_files)} ç¯‡æ–‡ç« éœ€è¦åŒæ­¥")

        results = []
        for idx, article_file in enumerate(article_files, 1):
            print(f"\n[{idx}/{len(article_files)}] ", end='')
            result = self.sync_article_with_interaction(article_file)
            if result:
                results.append(result)

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self._generate_sync_summary(results)

        return results

    def _generate_sync_summary(self, results: List[Dict]):
        """ç”ŸæˆåŒæ­¥æ±‡æ€»æŠ¥å‘Š"""
        if not results:
            print("æœªæˆåŠŸåŒæ­¥ä»»ä½•æ–‡ç« æ•°æ®")
            return

        total_reads = sum(r['interaction_metrics']['read_count'] for r in results)
        total_likes = sum(r['interaction_metrics']['like_count'] for r in results)
        avg_hot_index = sum(r['hot_index'] for r in results) / len(results)

        report = {
            'synced_at': datetime.now().isoformat(),
            'total_articles': len(results),
            'total_read_count': total_reads,
            'total_like_count': total_likes,
            'average_hot_index': round(avg_hot_index, 1),
            'top_articles': sorted(results, key=lambda x: x['hot_index'], reverse=True)[:3],
            'articles_data': results
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"sync-summary-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
        report_path = os.path.join(self.reports_dir, report_file)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\n\nğŸ¯ æ•°æ®åŒæ­¥æ±‡æ€»")
        print("="*60)
        print(f"æˆåŠŸåŒæ­¥: {len(results)} ç¯‡æ–‡ç« ")
        print(f"æ€»é˜…è¯»é‡: {total_reads:,}")
        print(f"æ€»ç‚¹èµæ•°: {total_likes:,}")
        print(f"å¹³å‡çƒ­åº¦: {avg_hot_index:.1f}/100")
        print(f"æŠ¥å‘Šä¿å­˜: {report_path}")
        print("="*60)

    def get_hot_articles(self, top_n: int = 5) -> List[Dict]:
        """è·å–çƒ­åº¦æœ€é«˜çš„æ–‡ç« """
        interaction_files = [
            f for f in os.listdir(self.articles_dir)
            if f.endswith('with-interaction.json')
        ]

        articles = []
        for file in interaction_files:
            with open(os.path.join(self.articles_dir, file), 'r') as f:
                data = json.load(f)
                articles.append(data)

        return sorted(articles, key=lambda x: x.get('hot_index', 0), reverse=True)[:top_n]

    def generate_weekly_report_data(self) -> Dict:
        """ç”Ÿæˆå‘¨æŠ¥æ•°æ®"""
        interaction_files = [
            f for f in os.listdir(self.articles_dir)
            if f.endswith('with-interaction.json')
        ]

        if not interaction_files:
            return {}

        articles = []
        total_metrics = {
            'read_count': 0,
            'like_count': 0,
            'share_count': 0,
            'comment_count': 0
        }

        for file in interaction_files:
            with open(os.path.join(self.articles_dir, file), 'r') as f:
                data = json.load(f)
                articles.append(data)

                # æ±‡æ€»æŒ‡æ ‡
                metrics = data.get('interaction_metrics', {})
                total_metrics['read_count'] += metrics.get('read_count', 0)
                total_metrics['like_count'] += metrics.get('like_count', 0)
                total_metrics['share_count'] += metrics.get('share_count', 0)
                total_metrics['comment_count'] += metrics.get('comment_count', 0)

        # è®¡ç®—çƒ­åº¦æ’è¡Œ
        hot_articles = sorted(articles, key=lambda x: x.get('hot_index', 0), reverse=True)[:10]

        # è·å–å†…å®¹åˆ†ææ±‡æ€»
        all_topics = []
        for article in articles:
            topics = article.get('content_analysis', {}).get('key_topics', [])
            all_topics.extend([(t['term'], t.get('weight', 0)) for t in topics])

        # èšåˆä¸»é¢˜çƒ­åº¦
        topic_counter = {}
        for term, weight in all_topics:
            topic_counter[term] = topic_counter.get(term, 0) + weight

        top_topics = sorted(topic_counter.items(), key=lambda x: x[1], reverse=True)[:15]

        return {
            'report_type': 'weekly_interaction',
            'generated_at': datetime.now().isoformat(),
            'total_articles': len(articles),
            'total_metrics': total_metrics,
            'hot_articles': hot_articles,
            'top_topics': [{'topic': t[0], 'score': round(t[1], 2)} for t in top_topics],
            'interaction_quality': self._calculate_overall_interaction_quality(articles),
            'market_insights': self._generate_market_insights(articles, total_metrics)
        }

    def _calculate_overall_interaction_quality(self, articles: List[Dict]) -> Dict:
        """è®¡ç®—æ•´ä½“äº’åŠ¨è´¨é‡"""
        total_score = sum(r.get('interaction_metrics', {}).get('quality_analysis', {}).get('interaction_score', 0)
                         for r in articles)
        avg_score = total_score / len(articles) if articles else 0
        total_reads = sum(r.get('interaction_metrics', {}).get('read_count', 0) for r in articles)
        total_likes = sum(r.get('interaction_metrics', {}).get('like_count', 0) for r in articles)

        like_rate = (total_likes / total_reads * 100) if total_reads > 0 else 0

        return {
            'average_interaction_score': round(avg_score, 1),
            'overall_like_rate': f'{like_rate:.2f}%',
            'total_readers': total_reads,
            'engagement_level': 'High' if avg_score >= 60 else 'Medium' if avg_score >= 40 else 'Low'
        }

    def _generate_market_insights(self, articles: List[Dict], metrics: Dict) -> List[str]:
        """ç”Ÿæˆå¸‚åœºæ´å¯Ÿ"""
        insights = []

        total_reads = metrics['read_count']
        read_millions = total_reads / 10000

        # é˜…è¯»é‡æ´å¯Ÿ
        if read_millions >= 100:
            insights.append(f"æ€»é˜…è¯»é‡è¶…è¿‡{read_millions:.0f}ä¸‡ï¼Œæ˜¾ç¤ºå¥¢ä¾ˆå“é¢†åŸŸå†…å®¹å…³æ³¨åº¦æé«˜")
        elif read_millions >= 50:
            insights.append(f"æ€»é˜…è¯»é‡è¾¾åˆ°{read_millions:.0f}ä¸‡ï¼Œåœ¨è¯¥å‚ç›´é¢†åŸŸè¡¨ç°ä¼˜ç§€")
        elif read_millions >= 10:
            insights.append(f"æ€»é˜…è¯»é‡{read_millions:.0f}ä¸‡ï¼Œä¿æŒç¨³å®šçš„è¯»è€…å…³æ³¨")

        # äº’åŠ¨è´¨é‡æ´å¯Ÿ
        like_rate = metrics['like_count'] / total_reads * 100 if total_reads > 0 else 0
        if like_rate >= 3:
            insights.append(f"å¹³å‡ç‚¹èµç‡{like_rate:.1f}%ï¼Œè¯»è€…å‚ä¸åº¦é«˜äºè¡Œä¸šå¹³å‡æ°´å¹³")
        elif like_rate >= 2:
            insights.append(f"å¹³å‡ç‚¹èµç‡{like_rate:.1f}%ï¼Œè¯»è€…å‚ä¸åº¦é€‚ä¸­")
        else:
            insights.append("äº’åŠ¨ç‡æœ‰å¾…æå‡ï¼Œå»ºè®®ä¼˜åŒ–å†…å®¹å½¢å¼å’Œå‘ˆç°æ–¹å¼")

        # è´¦å·æƒå¨åº¦åˆ†æ
        accounts = list(set(r.get('account_name', '') for r in articles))
        if len(accounts) >= 3:
            insights.append(f"æœ¬å‘¨åˆ†æäº†{len(accounts)}ä¸ªä¸åŒè´¦å·çš„å†…å®¹ï¼Œå±•ç°å¤šå…ƒåŒ–è§†è§’")

        # çƒ­é—¨ä¸»é¢˜æ´å¯Ÿ
        topics = {}
        for article in articles:
            article_topics = article.get('content_analysis', {}).get('key_topics', [])
            for topic in article_topics[:5]:  # åªå–å‰5ä¸ªä¸»é¢˜
                term = topic.get('term', '')
                if len(term) >= 2:  # è¿‡æ»¤å¤ªçŸ­çš„ä¸»é¢˜è¯
                    topics[term] = topics.get(term, 0) + topic.get('weight', 0)

        top_topics = sorted(topics.items(), key=lambda x: x[1], reverse=True)[:5]
        if top_topics:
            topic_names = [t[0] for t in top_topics]
            insights.append(f"æœ¬å‘¨çƒ­é—¨è®¨è®ºè¯é¢˜åŒ…æ‹¬ï¼š{', '.join(topic_names[:3])}")

        return insights


# ä½¿ç”¨ç¤ºä¾‹
def main():
    """æ¼”ç¤ºå®Œæ•´çš„æ•°æ®åŒæ­¥æµç¨‹"""

    print("ğŸ”„ å¼€å§‹å¾®ä¿¡å…¬ä¼—å·æ•°æ®åŒæ­¥æµç¨‹...")

    # 1. åˆå§‹åŒ–åŒæ­¥å™¨
    syncer = WeChatDataSync()

    # 2. åŒæ­¥å·²æœ‰æ–‡ç« æ•°æ®ï¼ˆæ·»åŠ äº’åŠ¨æŒ‡æ ‡ï¼‰
    synced_articles = syncer.sync_all_articles()

    # 3. ç”Ÿæˆå‘¨æŠ¥æ•°æ®
    if synced_articles:
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå‘¨æŠ¥æ•°æ®...")
        weekly_data = syncer.generate_weekly_report_data()

        # æ˜¾ç¤ºæœ¬å‘¨çƒ­ç‚¹
        print("\nğŸ”¥ æœ¬å‘¨çƒ­ç‚¹æ–‡ç« ï¼š")
        for i, article in enumerate(weekly_data['hot_articles'][:5], 1):
            metrics = article['interaction_metrics']
            print(f"  {i}. {article['title'][:50]}...")
            print(f"     é˜…è¯»é‡: {metrics['read_count']:,} | çƒ­åº¦æŒ‡æ•°: {article['hot_index']}")

        # æ˜¾ç¤ºçƒ­é—¨è¯é¢˜
        print(f"\nğŸ“ˆ æœ¬å‘¨çƒ­é—¨è¯é¢˜ï¼ˆå…±{len(weekly_data['top_topics'])}ä¸ªï¼‰ï¼š")
        for topic in weekly_data['top_topics'][:10]:
            print(f"   - {topic['topic']} (çƒ­åº¦: {topic['score']})")

        # ä¿å­˜æŠ¥å‘Š
        report_file = "weekly-interaction-report.json"
        report_path = os.path.join(syncer.reports_dir, report_file)

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(weekly_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… å‘¨æŠ¥æ•°æ®å·²ä¿å­˜åˆ°ï¼š{report_path}")
        print(f"ğŸ“Š æœ¬å‘¨æ€»é˜…è¯»é‡ï¼š{weekly_data['total_metrics']['read_count']:,}")
        print(f"ğŸ‘¥ å‚ä¸è´¦å·æ•°ï¼š {weekly_data['total_articles']}")

    else:
        print("æœªå‘ç°å¯åŒæ­¥çš„æ–‡ç« æ•°æ®")

    print("\nâœ… æ•°æ®åŒæ­¥æµç¨‹å®Œæˆï¼")

if __name__ == "__main__":
    main()